# Project 2: Continuous Control (Reacher) — DDPG (PyTorch)

This repository contains a **classic DDPG (Actor–Critic)** solution for Udacity DRLND Project 2 (Continuous Control / Reacher).

## Environment
- **State space**: 33 continuous values (positions, rotations, velocities, angular velocities).
- **Action space**: 4 continuous actions (torques), each in **[-1, 1]**.
- **Reward**: +0.1 for every step the hand is in the target location.
- **Solved criterion** (Udacity):
  - Version 1 (1 agent): average score >= **+30** over 100 consecutive episodes
  - Version 2 (20 agents): average of the 20 agent scores per episode, then average over 100 episodes >= **+30**

## Files
- `model.py` — Actor/Critic networks
- `ddpg_agent.py` — DDPG agent (replay buffer, OU noise, target networks)
- `train.py` — training script (saves weights + plot)
- `eval.py` — evaluate a saved actor
- `checkpoint_actor.pth`, `checkpoint_critic.pth` — trained weights (include these in submission)
- `scores.png` — learning curve (generated by `train.py`)
- `Report.md` — project report

## Getting Started

### 1) Install dependencies
You need Python 3 + PyTorch + NumPy + the Udacity Unity API (`unityagents`).

If you use Udacity's repo:
```bash
git clone https://github.com/udacity/deep-reinforcement-learning.git
cd deep-reinforcement-learning/python
pip install .
```

Also install:
```bash
pip install torch numpy matplotlib
```

### 2) Download the Unity environment
Download the Reacher environment from the Udacity classroom and place it locally, e.g.:
- `./Reacher_Windows_x86_64/Reacher.exe`

## Training
```bash
python train.py --env ./Reacher_Windows_x86_64/Reacher.exe
```

Outputs:
- `checkpoint_actor.pth`
- `checkpoint_critic.pth`
- `scores.npy`
- `scores.png`

## Evaluation (10 episodes)
```bash
python eval.py --env ./Reacher_Windows_x86_64/Reacher.exe --actor checkpoint_actor.pth --episodes 10
```

## Notes
- The provided config learns every 20 environment steps and performs 10 learning updates for stability (a common Udacity-style trick for Reacher).
- You can adjust hyperparameters in `DDPGConfig` inside `ddpg_agent.py`.
